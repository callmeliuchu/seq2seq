{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "318eaf9a-8b81-4742-8954-42ba371aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bada3884-cb7f-4201-89e8-e0765f767ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2eed65c9-a646-40f3-a6a3-896064186fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,output_size):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.Wx = nn.Linear(dim,dim)\n",
    "        self.Wh = nn.Linear(dim,dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.Wy = nn.Linear(dim,output_size)\n",
    "        \n",
    "    def hiddend_state(self, batch_size):\n",
    "        return torch.zeros(1, self.dim)\n",
    "        \n",
    "    def forward(self,x,h):\n",
    "        h = self.sigmoid(self.Wx(x) + self.Wh(h))\n",
    "        logits = self.Wy(h)\n",
    "        return h,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "820bd30b-77d7-40bc-90a3-20eb2a54e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = RNN(output_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a5bbaba5-7a3c-4da9-b1fa-7be76b23ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"First, should you want to get a PhD? I was in a fortunate position of knowing since young age that I really wanted a PhD. Unfortunately it wasn’t for any very well-thought-through considerations: First, I really liked school and learning things and I wanted to learn as much as possible, and second, I really wanted to be like Gordon Freeman from the game Half-Life (who has a PhD from MIT in theoretical physics). I loved that game. But what if you’re more sensible in making your life’s decisions? Should you want to do a PhD? There’s a very nice Quora thread and in the summary of considerations that follows I’ll borrow/restate several from Justin/Ben/others there. I’ll assume that the second option you are considering is joining a medium-large company (which is likely most common). Ask yourself if you find the following properties appealing:\n",
    "\n",
    "Freedom. A PhD will offer you a lot of freedom in the topics you wish to pursue and learn about. You’re in charge. Of course, you’ll have an adviser who will impose some constraints but in general you’ll have much more freedom than you might find elsewhere.\n",
    "\n",
    "Ownership. The research you produce will be yours as an individual. Your accomplishments will have your name attached to them. In contrast, it is much more common to “blend in” inside a larger company. A common feeling here is becoming a “cog in a wheel”.\n",
    "\n",
    "Exclusivity. There are very few people who make it to the top PhD programs. You’d be joining a group of a few hundred distinguished individuals in contrast to a few tens of thousands (?) that will join some company.\n",
    "\n",
    "Status. Regardless of whether it should be or not, working towards and eventually getting a PhD degree is culturally revered and recognized as an impressive achievement. You also get to be a Doctor; that’s awesome.\n",
    "\n",
    "Personal freedom. As a PhD student you’re your own boss. Want to sleep in today? Sure. Want to skip a day and go on a vacation? Sure. All that matters is your final output and no one will force you to clock in from 9am to 5pm. Of course, some advisers might be more or less flexible about it and some companies might be as well, but it’s a true first order statement.\n",
    "\n",
    "Maximizing future choice. Joining a PhD program doesn’t close any doors or eliminate future employment/lifestyle options. You can go one way (PhD -> anywhere else) but not the other (anywhere else -> PhD -> academia/research; it is statistically less likely). Additionally (although this might be quite specific to applied ML), you’re strictly more hirable as a PhD graduate or even as a PhD dropout and many companies might be willing to put you in a more interesting position or with a higher starting salary. More generally, maximizing choice for the future you is a good heuristic to follow.\n",
    "\n",
    "Maximizing variance. You’re young and there’s really no need to rush. Once you graduate from a PhD you can spend the next ~50 years of your life in some company. Opt for more variance in your experiences.\n",
    "\n",
    "Personal growth. PhD is an intense experience of rapid growth (you learn a lot) and personal self-discovery (you’ll become a master of managing your own psychology). PhD programs (especially if you can make it into a good one) also offer a high density of exceptionally bright people who will become your best friends forever.\n",
    "\n",
    "Expertise. PhD is probably your only opportunity in life to really drill deep into a topic and become a recognized leading expert in the world at something. You’re exploring the edge of our knowledge as a species, without the burden of lesser distractions or constraints. There’s something beautiful about that and if you disagree, it could be a sign that PhD is not for you.\n",
    "\n",
    "The disclaimer. I wanted to also add a few words on some of the potential downsides and failure modes. The PhD is a very specific kind of experience that deserves a large disclaimer. You will inevitably find yourself working very hard (especially before paper deadlines). You need to be okay with the suffering and have enough mental stamina and determination to deal with the pressure. At some points you will lose track of what day of the week it is and go on a diet of leftover food from the microkitchens. You’ll sit exhausted and alone in the lab on a beautiful, sunny Saturday scrolling through Facebook pictures of your friends having fun on exotic trips, paid for by their 5-10x larger salaries. You will have to throw away 3 months of your work while somehow keeping your mental health intact. You’ll struggle with the realization that months of your work were spent on a paper with a few citations while your friends do exciting startups with TechCrunch articles or push products to millions of people. You’ll experience identity crises during which you’ll question your life decisions and wonder what you’re doing with some of the best years of your life. As a result, you should be quite certain that you can thrive in an unstructured environment in the pursuit research and discovery for science. If you’re unsure you should lean slightly negative by default. Ideally you should consider getting a taste of research as an undergraduate on a summer research program before before you decide to commit. In fact, one of the primary reasons that research experience is so desirable during the PhD hiring process is not the research itself, but the fact that the student is more likely to know what they’re getting themselves into.\n",
    "\n",
    "I should clarify explicitly that this post is not about convincing anyone to do a PhD, I’ve merely tried to enumerate some of the common considerations above. The majority of this post focuses on some tips/tricks for navigating the experience once if you decide to go for it (which we’ll see shortly, below).\n",
    "\n",
    "Lastly, as a random thought I heard it said that you should only do a PhD if you want to go into academia. In light of all of the above I’d argue that a PhD has strong intrinsic value - it’s an end by itself, not just a means to some end (e.g. academic job).\n",
    "\n",
    "Getting into a PhD program: references, references, references. Great, you’ve decided to go for it. Now how do you get into a good PhD program? The first order approximation is quite simple - by far most important component are strong reference letters. The ideal scenario is that a well-known professor writes you a letter along the lines of: “Blah is in top 5 of students I’ve ever worked with. She takes initiative, comes up with her own ideas, and gets them to work.” The worst letter is along the lines of: “Blah took my class. She did well.” A research publication under your belt from a summer research program is a very strong bonus, but not absolutely required provided you have strong letters. In particular note: grades are quite irrelevant but you generally don’t want them to be too low. This was not obvious to me as an undergrad and I spent a lot of energy on getting good grades. This time should have instead been directed towards research (or at the very least personal projects), as much and as early as possible, and if possible under supervision of multiple people (you’ll need 3+ letters!). As a last point, what won’t help you too much is pestering your potential advisers out of the blue. They are often incredibly busy people and if you try to approach them too aggressively in an effort to impress them somehow in conferences or over email this may agitate them.\n",
    "\n",
    "Picking the school. Once you get into some PhD programs, how do you pick the school? It’s easy, join Stanford! Just kidding. More seriously, your dream school should 1) be a top school (not because it looks good on your resume/CV but because of feedback loops; top schools attract other top people, many of whom you will get to know and work with) 2) have a few potential advisers you would want to work with. I really do mean the “few” part - this is very important and provides a safety cushion for you if things don’t work out with your top choice for any one of hundreds of reasons - things in many cases outside of your control, e.g. your dream professor leaves, moves, or spontaneously disappears, and 3) be in a good environment physically. I don’t think new admits appreciate this enough: you will spend 5+ years of your really good years living near the school campus. Trust me, this is a long time and your life will consist of much more than just research.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26308e22-c272-4e0a-b294-249e3475a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "98ae1b4f-e988-496e-8a76-90c813546252",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {c:i for i,c in enumerate(chars)}\n",
    "id2char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dc4fd3ca-5462-44d5-8638-16e635b0b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [char2id[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "798022fe-bcd1-411e-b832-9526369c7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52749356-5fea-4e62-83d4-f40570beeba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c020f455-1335-4244-af0a-b891591a4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_x_y(tokens,seq_len):\n",
    "    i = random.randint(0,len(tokens)-seq_len-2)\n",
    "    seq = tokens[i:i+seq_len+1]\n",
    "    x = seq[:-1]\n",
    "    y = seq[1:]\n",
    "    return x,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dd76855e-ede0-412f-90c5-e2f25d451f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([45, 48, 26, 53, 11], [48, 26, 53, 11, 8])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_x_y(tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915a938-b648-4114-9980-e867b11d4de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79a045a9-9319-4a0c-b407-4eb66f3a2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4544612e-c8c3-43f3-8cc9-6d9ec1857229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2efa7749-49af-48be-86ea-b18c8d0f1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.LongTensor([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46395a00-61f7-49e4-9375-9def0483461b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89323f-ef0f-4784-a0e5-c0f2648d3309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f950f-e672-4ed5-af2f-462fbe31c473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684ea40-f073-441f-bd92-cb56a6e66f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe32063-2591-4b34-a3ed-133e3c8a1fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fa6b8937-113f-4b86-960c-18193b6a56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(rnn.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(3,vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6d1517e-a280-4a7f-8ee5-f4367198a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3527,  0.2079, -0.4963],\n",
      "        [-0.4626, -0.5038,  0.3728],\n",
      "        [-0.2397, -0.1835,  0.4137]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4653, -0.2420,  0.4055], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4135,  0.3189, -0.5262],\n",
      "        [ 0.3271,  0.2444, -0.1327],\n",
      "        [-0.1981, -0.1310, -0.3018]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3760,  0.1427, -0.5118], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5293, -0.3943,  0.1277],\n",
      "        [ 0.5323, -0.0481, -0.5705],\n",
      "        [-0.2903, -0.0178,  0.5487],\n",
      "        [ 0.2065, -0.1451, -0.1404],\n",
      "        [-0.5414, -0.2549,  0.5555],\n",
      "        [ 0.4923, -0.5752,  0.1124],\n",
      "        [-0.4802,  0.2080,  0.4433],\n",
      "        [ 0.2605,  0.2785,  0.2745],\n",
      "        [ 0.5128, -0.2100,  0.3447],\n",
      "        [-0.4318, -0.0462,  0.1224]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0057, -0.4894,  0.3295, -0.5227, -0.5501, -0.3207,  0.3060, -0.1866,\n",
      "         0.2037, -0.5422], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for n in rnn.parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7fec9702-fb55-4dcd-8966-f8d27039f2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>) tensor([[[0.4777, 0.4752, 0.4735]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    h = rnn.hiddend_state(1)\n",
    "    h,logits = rnn(x,h)\n",
    "    loss = criterion(logits.view(-1,logits.shape[-1]),y)\n",
    "    print(loss,h)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b184b-1717-440d-81e5-2b8ae47c8694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c8eb8-d48b-489f-a6bb-e7068f026a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a42c37-35d4-4636-bbf5-af9882ad4478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd37e99-bcc3-46ad-8174-1729964f72cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515a76b-7d8b-406b-9aa7-535a71b168ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7c5d6-a9ff-45b2-841e-25b41ab4d93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ad462-02fc-41cd-a905-fcdc53b8651a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch Environment",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
